{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0hoL4eb5kK9FHG8U0tx4q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52375/GENERATIVE_AI_2303A52375/blob/main/GEN_AI_ASSIGNMENT_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. Write Python code from scratch to find error metrics of deep learning model. Actual\n",
        " values and deep learning model predicted values are shown in Table 1. Also compare the results\n",
        " with the outcomes of libraries\n",
        " YActual YP red\n",
        " 20 20.5\n",
        " 30 30.3\n",
        " 40 40.2\n",
        " 50 50.6\n",
        " 60 60.7\n",
        " Tabela 1: YActual Vs. YP red"
      ],
      "metadata": {
        "id": "tAc0SMw5co2j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "h5nFxuOVcABb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "Y_actual = [20, 30, 40, 50, 60]\n",
        "Y_pred = [20.5, 30.3, 40.2, 50.6, 60.7]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mae(y_actual, y_pred):\n",
        "  absolute_errors = [abs(a - p) for a, p in zip(y_actual, y_pred)]\n",
        "  return sum(absolute_errors) / len(y_actual)"
      ],
      "metadata": {
        "id": "Wpfto2z8cTme"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mse(y_actual, y_pred):\n",
        "    squared_errors = [(a - p) ** 2 for a, p in zip(y_actual, y_pred)]\n",
        "    return sum(squared_errors) / len(y_actual)\n"
      ],
      "metadata": {
        "id": "WoaOA8cxcUsd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_rmse(y_actual, y_pred):\n",
        "    mse = calculate_mse(y_actual, y_pred)\n",
        "    return mse ** 0.5\n",
        "mae_manual = calculate_mae(Y_actual, Y_pred)\n",
        "mse_manual = calculate_mse(Y_actual, Y_pred)\n",
        "rmse_manual = calculate_rmse(Y_actual, Y_pred)"
      ],
      "metadata": {
        "id": "5nVqT6MscV4O"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_library = mean_absolute_error(Y_actual, Y_pred)\n",
        "mse_library = mean_squared_error(Y_actual, Y_pred)\n",
        "rmse_library = np.sqrt(mse_library)\n"
      ],
      "metadata": {
        "id": "5FPN8G7VcX9u"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Manual Calculations:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_manual}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_manual}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_manual}\")\n",
        "\n",
        "print(\"\\nLibrary Calculations:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_library}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_library}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_library}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZagZMwBucdhx",
        "outputId": "698f70cf-7e81-4bd2-8807-0e142585cf84"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Calculations:\n",
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n",
            "\n",
            "Library Calculations:\n",
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write python code from scratch to find evaluation metrics of deep learning model.\n",
        "Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
        "results with outcome of libraries\n",
        "YActual YP red\n",
        "0 0 1 1 2 0\n",
        "0 0 1 0 2 0\n",
        "0 1 1 2 2 1\n",
        "0 2 1 0 2 2\n",
        "0 2 1 2 2 2\n",
        "Tabela 2: YActual Vs. YP red"
      ],
      "metadata": {
        "id": "_fYxvhcVct2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "Y_actual = [\n",
        "    [0, 0, 1],\n",
        "    [1, 2, 0],\n",
        "    [0, 0, 1],\n",
        "    [0, 1, 1],\n",
        "    [2, 2, 1],\n",
        "    [0, 2, 1]\n",
        "]\n",
        "Y_pred = [\n",
        "    [0, 0, 1],\n",
        "    [0, 2, 0],\n",
        "    [0, 1, 1],\n",
        "    [2, 0, 1],\n",
        "    [2, 2, 2],\n",
        "    [0, 2, 1]\n",
        "]"
      ],
      "metadata": {
        "id": "0XvYlzKLcgfI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_actual_flat = [label for row in Y_actual for label in row]\n",
        "y_pred_flat = [label for row in Y_pred for label in row]\n",
        "def calculate_accuracy(y_actual, y_pred):\n",
        "    correct_predictions = sum(1 for a, p in zip(y_actual, y_pred) if a == p)\n",
        "    return correct_predictions / len(y_actual)"
      ],
      "metadata": {
        "id": "m1e9NmyFchx4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_precision(y_actual, y_pred):\n",
        "    true_positive = sum(1 for a, p in zip(y_actual, y_pred) if a == p and p == 1)\n",
        "    predicted_positive = sum(1 for p in y_pred if p == 1)\n",
        "    return true_positive / predicted_positive if predicted_positive > 0 else 0\n"
      ],
      "metadata": {
        "id": "Pz61qWlYci3L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_recall(y_actual, y_pred):\n",
        "    true_positive = sum(1 for a, p in zip(y_actual, y_pred) if a == p and p == 1)\n",
        "    actual_positive = sum(1 for a in y_actual if a == 1)\n",
        "    return true_positive / actual_positive if actual_positive > 0 else 0\n",
        "def calculate_f1(precision, recall):\n",
        "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n"
      ],
      "metadata": {
        "id": "cU0lpCxjcjvZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_manual = calculate_accuracy(y_actual_flat, y_pred_flat)\n",
        "precision_manual = calculate_precision(y_actual_flat, y_pred_flat)\n",
        "recall_manual = calculate_recall(y_actual_flat, y_pred_flat)\n",
        "f1_manual = calculate_f1(precision_manual, recall_manual)\n",
        "\n",
        "accuracy_library = accuracy_score(y_actual_flat, y_pred_flat)\n",
        "precision_library = precision_score(y_actual_flat, y_pred_flat, average='macro')\n",
        "recall_library = recall_score(y_actual_flat, y_pred_flat, average='macro')\n",
        "f1_library = f1_score(y_actual_flat, y_pred_flat, average='macro')\n"
      ],
      "metadata": {
        "id": "C4ulERcQcku1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Manual Calculations:\")\n",
        "print(f\"Accuracy: {accuracy_manual}\")\n",
        "print(f\"Precision: {precision_manual}\")\n",
        "print(f\"Recall: {recall_manual}\")\n",
        "print(f\"F1 Score: {f1_manual}\")\n",
        "\n",
        "print(\"\\nLibrary Calculations:\")\n",
        "print(f\"Accuracy: {accuracy_library}\")\n",
        "print(f\"Precision: {precision_library}\")\n",
        "print(f\"Recall: {recall_library}\")\n",
        "print(f\"F1 Score: {f1_library}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjbnHeGOcle7",
        "outputId": "5ba43e14-154f-49c4-b81b-ecb78cfb068c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Calculations:\n",
            "Accuracy: 0.7222222222222222\n",
            "Precision: 0.8\n",
            "Recall: 0.5714285714285714\n",
            "F1 Score: 0.6666666666666666\n",
            "\n",
            "Library Calculations:\n",
            "Accuracy: 0.7222222222222222\n",
            "Precision: 0.726984126984127\n",
            "Recall: 0.7619047619047619\n",
            "F1 Score: 0.7269841269841271\n"
          ]
        }
      ]
    }
  ]
}